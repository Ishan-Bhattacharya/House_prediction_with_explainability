{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# House Price Prediction with Explainable AI\n",
        "\n",
        "This project aims to predict house prices using a neural network enhanced with city embeddings, while ensuring model interpretability through SHAP (SHapley Additive exPlanations). The goal is to build a robust predictive model and understand which features most influence price predictions, making it valuable for real estate analytics or data science applications. The notebook also includes a baseline linear regression model for comparison."
      ],
      "metadata": {
        "id": "vUikQWG9hW2d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "2aAr4UC3WBII",
        "outputId": "25d4af59-6fd6-4ac6-dbe9-b3ca0f075f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==2.0.3\n",
            "  Using cached pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.17.0)\n",
            "Using cached pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dice-ml 0.11 requires pandas<2.0.0, but you have pandas 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "a584b39038b147b4a94365dc0dfea09c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#run this the firsrt time you run this notebook and then restart kernel\n",
        "\n",
        "!pip install --upgrade pandas==2.0.3 numpy==1.25.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"The dataset is sourced from Kaggle ([House Data by shree1992](https://www.kaggle.com/shree1992/housedata)), containing approximately 4,600 records of house sales in the King County area. Key features include `sqft_living` (living space square footage), `city`, `bedrooms`, `bathrooms`, `yr_built`, and `price` (the target variable). The data is preprocessed to handle categorical variables (e.g., `city`) and engineer new features like `Age` (current year minus `yr_built`) and `Renovated` (binary indicator for renovations).\""
      ],
      "metadata": {
        "id": "DrZOiGNDhNmN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9sxCeJwT5mQ",
        "outputId": "1314d0f5-db9d-4196-8ad5-57ac987061a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.13.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Requirement already satisfied: dice-ml in /usr/local/lib/python3.11/dist-packages (0.11)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from dice-ml) (4.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from dice-ml) (1.25.2)\n",
            "Collecting pandas<2.0.0 (from dice-ml)\n",
            "  Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from dice-ml) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dice-ml) (4.67.1)\n",
            "Requirement already satisfied: raiutils>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dice-ml) (0.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0->dice-ml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0.0->dice-ml) (2025.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from raiutils>=0.4.0->dice-ml) (2.32.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from raiutils>=0.4.0->dice-ml) (1.14.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->dice-ml) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->dice-ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->dice-ml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->dice-ml) (0.24.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dice-ml) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->dice-ml) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0->dice-ml) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema->dice-ml) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2025.1.31)\n",
            "Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n",
            "Data source import complete.\n",
            "GPU Available:  []\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# 1. IMPORTING IMPORTANT MODULES AND DATASET\n",
        "# --------------------------------------------\n",
        "!pip install shap\n",
        "\n",
        "import dice_ml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "import shap\n",
        "\n",
        "\n",
        "#importing the data\n",
        "\n",
        "import kagglehub\n",
        "shree1992_housedata_path = kagglehub.dataset_download('shree1992/housedata')\n",
        "\n",
        "print('Data source import complete.')\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "data = pd.read_csv('/kaggle/input/housedata/data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing\n",
        "\n",
        "Data preparation involves several steps to ensure compatibility with the neural network:,\n",
        "- **Shuffling and Splitting**: The dataset is shuffled and split into 80% training and 20% test sets for robust evaluation.,\n",
        "- **Encoding**: The `city` feature is encoded using `LabelEncoder` to convert it into numerical values suitable for embedding,\n",
        "- **Feature Engineering**: `Age` is calculated as 2025 minus `yr_built`, and `Renovated` is a binary flag (1 if `yr_renovated` > 0, 0 otherwise).,\n",
        "- **Column Removal**: Irrelevant features (`date`, `street`, `statezip`, `country`) are dropped to reduce noise.,\n",
        "This preprocessing ensures the data is clean and structured for modeling."
      ],
      "metadata": {
        "id": "d04DCWOrhksf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wnSsFFoZWXCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b074e8-805a-4ad2-c2ac-0a9a7c1f1f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-0ac413efc7eb>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train.loc[:, 'city'] = encoder.fit_transform(X_train['city'])\n",
            "<ipython-input-28-0ac413efc7eb>:19: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  X_train.loc[:, 'city'] = encoder.fit_transform(X_train['city'])\n",
            "<ipython-input-28-0ac413efc7eb>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test.loc[:, 'city'] = encoder.transform(X_test['city'])\n",
            "<ipython-input-28-0ac413efc7eb>:20: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  X_test.loc[:, 'city'] = encoder.transform(X_test['city'])\n",
            "<ipython-input-28-0ac413efc7eb>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train['Age'] = 2025 - X_train['yr_built']\n",
            "<ipython-input-28-0ac413efc7eb>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test['Age'] = 2025 - X_test['yr_built']\n"
          ]
        }
      ],
      "source": [
        "#2. PREPROCESSING 1\n",
        "\n",
        "X = data.drop('price', axis = 1)\n",
        "y = data['price']\n",
        "\n",
        "X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n",
        "\n",
        "X_train = X_shuffled.iloc[: int(0.8 * len(data))]\n",
        "X_test = X_shuffled.iloc[int(0.8 * len(data)):]\n",
        "y_train = y_shuffled.iloc[:int(0.8 * len(data))]\n",
        "y_test = y_shuffled.iloc[int(0.8 * len(data)):]\n",
        "\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "X_train.loc[:, 'city'] = encoder.fit_transform(X_train['city'])\n",
        "X_test.loc[:, 'city'] = encoder.transform(X_test['city'])\n",
        "\n",
        "X_train['Age'] = 2025 - X_train['yr_built']\n",
        "X_test['Age'] = 2025 - X_test['yr_built']\n",
        "\n",
        "X_train = X_train.drop(['yr_built'], axis = 1)\n",
        "X_test = X_test.drop(['yr_built'], axis = 1)\n",
        "\n",
        "X_train['Renovated'] = np.where(X_train['yr_renovated'] == 0, 0, 1)\n",
        "X_test['Renovated'] = np.where(X_test['yr_renovated'] == 0, 0, 1)\n",
        "\n",
        "X_train = X_train.drop(['yr_renovated'], axis = 1)\n",
        "X_test = X_test.drop(['yr_renovated'], axis = 1)\n",
        "\n",
        "X_train = X_train.drop(['date','street','statezip', 'country'], axis = 1)\n",
        "X_test = X_test.drop(['date','street','statezip', 'country'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6U5ez5JoT5mb"
      },
      "outputs": [],
      "source": [
        "#4. EMBEDDING LAYER\n",
        "city = data['city'].nunique()\n",
        "\n",
        "city_input = Input(shape=(1,), name='city_input')\n",
        "city_embeds = Embedding(input_dim = city, output_dim = 8)(city_input)\n",
        "city_flat = Flatten()(city_embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JbT577b8T5mb"
      },
      "outputs": [],
      "source": [
        "#5. PREPROCESSING 2\n",
        "\n",
        "X_train_city = X_train['city']\n",
        "X_train_num = X_train.drop(['city'], axis = 1)\n",
        "\n",
        "X_train_num_variance = X_train_num.var()\n",
        "low_variance_cols = X_train_num_variance[X_train_num_variance < 1e-6].index\n",
        "X_train_num = X_train_num.drop(columns=low_variance_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Modeling\n",
        "\"he predictive model is a neural network designed to handle both categorical and numerical features:\n",
        "- **City Embedding**: The `city` feature is embedded into an 8-dimensional space using an `Embedding` layer, capturing geographic patterns.\n",
        "- **Numerical Input**: Other features (e.g., `sqft_living`, `Age`) are fed through a dense layer.\n",
        "- **Architecture**: The model concatenates these inputs, processes them through a dense layer (128 units with ReLU activation), and outputs a price prediction.\n",
        "- **Baseline**: A linear regression model serves as a benchmark to compare the neural network’s performance.\n",
        "The neural network is trained to minimize mean squared error, with ongoing efforts to evaluate its accuracy against the baseline."
      ],
      "metadata": {
        "id": "cLu6j9K4iGUb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "3jOAukoBT5mc",
        "outputId": "a8826b59-106c-42e7-de5a-e2282eee0df3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
  ...(truncated 276446 characters)...hdl20pxtpaWnk6upKBgYG1LdvX1qwYAHV1NSolJ+IaPLkyQSAHB0dVZbZ0NBAcXFxNGjQIOrZsyeZmJiQi4sLzZ8/n86fP6/0nqpCTV69epXGjx9P5ubmZGJiQr6+vnTq1CmV/WPfvn0kkUhIIBCQnZ0dxcbGklQqVQqfS/QmRG1ycjJ5e3uTkZERGRkZkVgsppkzZ9LRo0dVvlt7sneVXZGH6ywuLqaxY8eSkZERmZub06xZs+jRo0ectM3NzbR+/XoaMGAACQQC6tevH0VHR9ONGzd4Q1a2tLRQSkoKeXh4kKGhIZmYmJBEIqHY2FhOuoyMDHJ1daUePXq0qw+tuXLlCn388ccK/XZxcaGNGzdywrOqeueO6qktqvpke6E6VYVfTUtLoyFDhpCBgQGZmpqSv78/nTp1SildS0sLrV27lvr160cCgYDc3Nxoz549KmWprq6mFStWkKOjIxkYGJBQKKRBgwbR4sWLOSGxNQ25qm49ExEdPnyY3N3dSSAQkLW1NcXExFBZWRlvHR0+fJg8PDzIwMCAACjCi7YOcZqcnExisZgEAgGJxWL6/vvveWXMyMggJycn6tGjB9nb29PGjRvp+PHjvKFSNdU1VfrTXihWvhC4Bw8eJE9PTzIyMiJLS0sKCwujyspK3rTNzc0UGxtLdnZ2JBAISCKR0E8//UTLly8nAPT48eMO5SNS1m9V+nrhwgUaMWIEGRkZEQCO3r58+ZLmzZtHvXr1IkNDQxo1ahSdOXNGZbkHDhxQ6ICtrS2tXr2aCgsLeetKE7t48uRJAkB5eXm879od6BC9hVNSDAYDALBw4UIUFhbi1q1bnNXKyMhInDx5kvc2UcY/k4yMDMyZMwcVFRWcm3OTk5OxatUqRXQfdVGlG/8GEhISsGLFCpw7dw4+Pj7vWhy1sLe3h729PedWbwbjXXHy5EmMHTsW6enpat3A/m9iypQpKCoqQl1dXbcEZ/gnExQUhPv37+PSpUtv7awMO2PBYLxF4uLiUFNTg/T09HctCqMbaGxsRHx8PKKjozWaVAD/Dt1oampSOr9SX1+PrVu3wtLSknOHCYPBYGgC35nEkpIS5Ofnw8/P7183qbh8+TJyc3ORkJDwVg/gszMWDMZbxMrKCi9evHjXYjC6CUNDQ1RVVXXqb/8NulFeXo6JEydi+vTpcHBwQFVVFXbv3o2Kigps27ZN6U4IBoPBUJfdu3cjMzMTH330EUQiEcrKypCWlgaBQIC4uLh3Ld5bR35m6G3DJhYMBoPBeCuIRCL4+PggOzsbT548gb6+PiQSCeLj4xEaGvquxWMwGP/DeHp64uDBg/jhhx/w7NkzmJqaws/PD99++60ichij+2FnLBgMBoPBYDAYDIbWsDMWDAaDwWAwGAwGQ2vYxILBYDAYDAaDwWBoDZtYMBgMBoPBYDAYDK1hEwsGg8FgMBgMBoOhNWxiwWAwGAwGg8FgMLSGTSwYDAaDwWAwGAyG1rCJBYPBYDAYDAaDwdAaNrFgMBgMBoPBYDAYWvMfEhgUFR2MLM4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#9. SHAP EXPLANATIONS\n",
        "\n",
        "preds = model.predict([X_test_city, X_test_num])\n",
        "\n",
        "# Combine categorical and numerical for SHAP\n",
        "X_train_city_2d = np.array(X_train_city).reshape(-1, 1)\n",
        "X_test_city_2d = np.array(X_test_city).reshape(-1, 1)\n",
        "\n",
        "# Drop low-variance columns\n",
        "X_train_num_variance = X_train_num.var()\n",
        "low_variance_cols = X_train_num_variance[X_train_num_variance < 1e-6].index\n",
        "X_train_num = X_train_num.drop(columns=low_variance_cols)\n",
        "X_test_num = X_test_num.drop(columns=low_variance_cols)\n",
        "\n",
        "# Rebuild full datasets\n",
        "X_train_full = pd.concat([pd.DataFrame(X_train_city_2d, columns=['city']), X_train_num], axis=1)\n",
        "X_test_full = pd.concat([pd.DataFrame(X_test_city_2d, columns=['city']), X_test_num], axis=1)\n",
        "\n",
        "# Impute missing values (same imputer as before)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_full_imputed = imputer.fit_transform(X_train_full)\n",
        "X_test_full_imputed = imputer.transform(X_test_full)\n",
        "\n",
        "X_train_full = pd.DataFrame(X_train_full_imputed, columns=X_train_full.columns)\n",
        "X_test_full = pd.DataFrame(X_test_full_imputed, columns=X_train_full.columns)\n",
        "\n",
        "def predict_fn(data):\n",
        "    city_data = data[:, 0].astype(int).reshape(-1, 1)\n",
        "    num_data = data[:, 1:]\n",
        "    predictions = model.predict([city_data, pd.DataFrame(num_data, columns=X_train_num.columns)])[0]\n",
        "    return predictions.ravel()\n",
        "\n",
        "background = X_train_full.iloc[:50, :].values\n",
        "\n",
        "explainer = shap.KernelExplainer(predict_fn, background)\n",
        "\n",
        "shap_values = explainer.shap_values(X_test_full.iloc[:100].values)\n",
        "\n",
        "# Wrap shap_values with Explanation object\n",
        "shap_explanation = shap.Explanation(values=shap_values,\n",
        "                                  base_values=explainer.expected_value,\n",
        "                                  data=X_test_full.iloc[:100].values,\n",
        "                                  feature_names=X_test_full.columns)\n",
        "\n",
        "# Use the Explanation object for the waterfall plot\n",
        "shap.plots.waterfall(shap_explanation[0]) # Pass the first explanation from the list\n",
        "\n",
        "\n",
        "shap.summary_plot(shap_values, X_test_full.iloc[:100], plot_type=\"bar\", feature_names=X_train_full.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Baseline Comparison\n",
        "A linear regression model is implemented as a baseline to assess the neural network’s added value:\n",
        "- **Approach**: The baseline uses the same preprocessed features and imputes missing values with means.\n",
        "- **Metrics**: Evaluated with RMSE (Root Mean Squared Error) and R² (Coefficient of Determination).\n",
        "- **Results**: The baseline RMSE of approximately 399,698 and R² of -0.01 indicate poor fit, suggesting the neural network may offer improvement with proper training.\n",
        "This comparison highlights the need for a more complex model to capture non-linear relationships in the data."
      ],
      "metadata": {
        "id": "MXA4OLCJisHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10. BASELINE\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "\n",
        "# Combine categorical and numerical for baseline model\n",
        "X_train_city_2d = np.array(X_train_city).reshape(-1, 1)\n",
        "X_train_full = pd.concat([pd.DataFrame(X_train_city_2d, columns=['city']), X_train_num], axis=1)\n",
        "\n",
        "# Drop low-variance columns from X_train_full as well\n",
        "# Ensure low_variance_cols is calculated based on the combined DataFrame\n",
        "X_train_full_variance = X_train_full.var()\n",
        "low_variance_cols = X_train_full_variance[X_train_full_variance < 1e-6].index\n",
        "X_train_full = X_train_full.drop(columns=low_variance_cols, errors='ignore')\n",
        "\n",
        "columns = X_train_full.columns\n",
        "# Create X_test_full with the same columns as X_train_full\n",
        "X_test_city_2d = np.array(X_test_city).reshape(-1, 1)\n",
        "X_test_full = pd.concat([pd.DataFrame(X_test_city_2d, columns=['city']), X_test_num], axis=1)\n",
        "\n",
        "# Ensure X_test_full has the same columns and is filtered for low variance similarly to X_train_full\n",
        "X_test_full = X_test_full[X_train_full.columns]  # Ensure same columns as X_train_full\n",
        "X_test_full = X_test_full.drop(columns=low_variance_cols, errors='ignore') # Drop low variance columns\n",
        "\n",
        "# Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean') # Create an imputer instance\n",
        "X_train_full = imputer.fit_transform(X_train_full) # Fit and transform on training data\n",
        "X_test_full = imputer.transform(X_test_full) # Transform test data\n",
        "\n",
        "X_train_full = pd.DataFrame(X_train_full, columns=columns)\n",
        "X_test_full = pd.DataFrame(X_test_full, columns=columns)\n",
        "\n",
        "# Before fitting, ensure X_train_full and y_train have the same number of samples\n",
        "# This is likely where the issue is; adjust X_train_full or y_train to match lengths\n",
        "X_train_full = X_train_full.iloc[:len(y_train)] # Adjust X_train_full to match y_train length\n",
        "# Alternatively, you might need to adjust y_train:\n",
        "# y_train = y_train.iloc[:len(X_train_full)]\n",
        "\n",
        "baseline_model = LinearRegression()\n",
        "baseline_model.fit(X_train_full, y_train)\n",
        "\n",
        "baseline_preds = baseline_model.predict(X_test_full)\n",
        "\n",
        "# Ensure y_test and baseline_preds have the same length before calculating RMSE\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_preds[:len(y_test)])) # Slice baseline_preds to match y_test length\n",
        "baseline_r2 = r2_score(y_test, baseline_preds[:len(y_test)]) # Slice baseline_preds to match y_test length\n",
        "\n",
        "print(f\"Baseline Model - RMSE: {baseline_rmse:.2f}, R²: {baseline_r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi0PPYXEueny",
        "outputId": "d2c76483-c904-4eca-ce60-594839916f47"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Model - RMSE: 399698.93, R²: -0.01\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 46927,
          "sourceId": 85203,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}